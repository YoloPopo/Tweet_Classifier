{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TF-IDF, BoW models:\n",
    "- Preprocess data (tokenization, lemmatization/stemming, …).\n",
    "- Describe why did you choose particular tokenizer/stemmer, etc.\n",
    "- Apply TF-IDF, BoW vectorizers\n",
    "- Use ML model (logistic regression, SVM, etc) to classify texts\n",
    "- Tune hyperparameters to get the best model\n",
    "- Test the best model with Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Install Dependencies & Imports**\n",
    "**Explanation**:  \n",
    "- **Dependencies**:  \n",
    "  - **nltk**: For preprocessing tasks such as tokenization, lemmatization, and stopwords removal.  \n",
    "  - **contractions**: To expand contractions (e.g., \"can't\" → \"cannot\").  \n",
    "  - **sklearn**: For vectorization (TF-IDF/BoW), machine learning models (Logistic Regression, SVM, etc.), and hyperparameter tuning.  \n",
    "- **Key Imports**:  \n",
    "  - `TfidfVectorizer`, `CountVectorizer`: Convert text into numerical features for model training.  \n",
    "  - `LogisticRegression`, `LinearSVC`, `MultinomialNB`: Linear and probabilistic classifiers for text classification.  \n",
    "  - `GradientBoostingClassifier`: Tree-based ensemble model to evaluate performance against linear methods.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (3.9.1)\n",
      "Requirement already satisfied: contractions in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (0.1.73)\n",
      "Requirement already satisfied: joblib in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: click in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\askeladd\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Askeladd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Askeladd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Askeladd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Askeladd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk contractions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Text Preprocessing Class**\n",
    "**Explanation**:  \n",
    "This class implements a comprehensive text preprocessing pipeline for disaster message classification. Key components include:  \n",
    "\n",
    "#### **Tokenizer**  \n",
    "- **Choice**: `word_tokenize` from NLTK.  \n",
    "  - **Reason**: Efficiently splits text into individual words and punctuation, handling edge cases like contractions and hyphenated words.  \n",
    "\n",
    "#### **Lemmatizer**  \n",
    "- **Choice**: `WordNetLemmatizer`.  \n",
    "  - **Reason**: Provides context-aware base forms (e.g., \"running\" → \"run\", \"better\" → \"good\"). Unlike stemming (e.g., Porter Stemmer), lemmatization avoids over-reduction of words.  \n",
    "\n",
    "#### **Stopwords Removal**  \n",
    "- **Base List**: NLTK's English stopwords (e.g., \"the\", \"and\").  \n",
    "- **Custom Additions**:  \n",
    "  - Social media noise: \"http\", \"https\", \"com\", \"www\", \"user\", \"rt\" (to remove URLs, mentions, and retweet indicators).  \n",
    "\n",
    "#### **Custom Preprocessing Steps**:  \n",
    "1. **Clean Text**:  \n",
    "   - **Regex Patterns**:  \n",
    "     - `r'http\\S+|@\\w+'`: Removes URLs and mentions.  \n",
    "     - `r'#(\\w+)'`: Strips \"#\" from hashtags (e.g., \"#earthquake\" → \"earthquake\").  \n",
    "     - `r'[^a-zA-Z0-9]'`: Replaces non-alphanumeric characters with spaces.  \n",
    "   - **Lowercase & Trim**: Ensures uniformity (e.g., \"RUNNING\" → \"running\") and removes leading/trailing whitespace.  \n",
    "\n",
    "2. **Expand Contractions**:  \n",
    "   - **Library**: `contractions.fix()` converts informal contractions (e.g., \"can't\" → \"cannot\").  \n",
    "\n",
    "3. **Token Filtering**:  \n",
    "   - **Stopwords Check**: Removes common words (including custom additions).  \n",
    "   - **Length Filter**: Excludes single-character words (e.g., \"a\", \"I\").  \n",
    "   - **Lemmatization**: Reduces words to their base form before joining into cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.update(['http', 'https', 'com', 'www', 'user', 'rt'])\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'http\\S+|@\\w+', '', text)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "        return text.lower().strip()\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.expand_contractions(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        return ' '.join([\n",
    "            self.lemmatizer.lemmatize(word)\n",
    "            for word in tokens\n",
    "            if word not in self.stop_words and len(word) > 1\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### **3. Data Loading & Preprocessing**\n",
    "**Explanation**:  \n",
    "- **Data Overview**:  \n",
    "  - `train.csv` contains messages labeled as disaster-related (`target=1`) or non-disaster (`target=0`).  \n",
    "  - `test.csv` is used for final predictions and lacks the `target` column.  \n",
    "\n",
    "- **Steps Performed**:  \n",
    "  1. **Loading Data**:  \n",
    "     - Use `pd.read_csv()` to load raw training and test datasets.  \n",
    "\n",
    "  2. **Preprocessing Pipeline**:  \n",
    "     - **Clean Text**: Remove URLs, mentions, hashtags, and non-alphanumeric characters using `clean_text()`.  \n",
    "     - **Expand Contractions**: Convert informal contractions (e.g., \"can't\" → \"cannot\") via `expand_contractions()`.  \n",
    "     - **Lemmatization & Filtering**: Tokenize text, lemmatize words to their base form, and filter out stopwords and short words using `preprocess()`.  \n",
    "     - The cleaned text is stored in a new column `cleaned` for both datasets.  \n",
    "\n",
    "  3. **Train-Validation Split**:  \n",
    "     - Split the training data into 80% training (`X_train_raw`, `y_train`) and 20% validation (`X_val_raw`, `y_val`).  \n",
    "     - **Stratification**: Ensures the class distribution (disaster/non-disaster) in the validation set matches the training data.  \n",
    "     - `random_state=42` guarantees reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "train_df['cleaned'] = train_df['text'].apply(preprocessor.clean_text).apply(preprocessor.preprocess)\n",
    "test_df['cleaned'] = test_df['text'].apply(preprocessor.clean_text).apply(preprocessor.preprocess)\n",
    "\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    train_df['cleaned'], train_df['target'],\n",
    "    test_size=0.2, stratify=train_df['target'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model Pipelines**\n",
    "**Explanation**:  \n",
    "This step defines multiple machine learning pipelines to compare different vectorization and classification approaches:  \n",
    "\n",
    "#### **Pipeline Components**:  \n",
    "1. **Vectorizers**:  \n",
    "   - **TF-IDF (`TfidfVectorizer`)**: Captures term importance by weighting words based on their frequency in documents vs. the corpus.  \n",
    "   - **Bag-of-Words (`CountVectorizer`)**: Represents text as raw word counts, emphasizing term frequency.  \n",
    "\n",
    "2. **Classifiers**:  \n",
    "   - **Logistic Regression (LR)**: Linear model with regularization to handle high-dimensional text features.  \n",
    "   - **Support Vector Machine (SVM)**: Maximizes margin between classes using hinge loss.  \n",
    "   - **Gradient Boosting (GB)**: Tree-based ensemble for comparison, despite potential challenges with sparse text data.  \n",
    "   - **Multinomial Naive Bayes (MNB)**: Probabilistic model suited for discrete features (common in text tasks).  \n",
    "\n",
    "#### **Key Design Choices**:  \n",
    "- **Class Weight Balancing**:  \n",
    "  - `class_weight='balanced'` in LR and SVM ensures the model accounts for imbalanced disaster/non-disaster classes.  \n",
    "- **Solver & Regularization**:  \n",
    "  - Logistic Regression uses `liblinear` solver for small datasets and L1/L2 penalties for feature selection/overfitting control.  \n",
    "- **Gradient Boosting Inclusion**:  \n",
    "  - Added to test if tree-based models (which handle non-linear relationships) can outperform linear methods, despite text data's high dimensionality.  \n",
    "\n",
    "#### **Pipeline Structure**:  \n",
    "Each pipeline combines a vectorizer and classifier in a `sklearn.pipeline.Pipeline` for seamless processing (vectorization → classification).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipelines():\n",
    "    return {\n",
    "        'tfidf_lr': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000))\n",
    "        ]),\n",
    "        'tfidf_svm': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LinearSVC(class_weight='balanced', dual=False, max_iter=10000))\n",
    "        ]),\n",
    "        'bow_lr': Pipeline([\n",
    "            ('bow', CountVectorizer()),\n",
    "            ('clf', LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000))\n",
    "        ]),\n",
    "        'bow_svm': Pipeline([\n",
    "            ('bow', CountVectorizer()),\n",
    "            ('clf', LinearSVC(class_weight='balanced', dual=False, max_iter=10000))\n",
    "        ]),\n",
    "        'tfidf_gb': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', GradientBoostingClassifier())\n",
    "        ]),\n",
    "        'bow_gb': Pipeline([\n",
    "            ('bow', CountVectorizer()),\n",
    "            ('clf', GradientBoostingClassifier())\n",
    "        ]),\n",
    "        'tfidf_mnb': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', MultinomialNB())\n",
    "        ]),\n",
    "        'bow_mnb': Pipeline([\n",
    "            ('bow', CountVectorizer()),\n",
    "            ('clf', MultinomialNB())\n",
    "        ])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Hyperparameter Grids for Models**\n",
    "**Explanation**:  \n",
    "This step defines hyperparameter grids to optimize model performance across all pipelines. Hyperparameters are tuned using **GridSearchCV** with **StratifiedKFold** cross-validation to handle class imbalance.  \n",
    "\n",
    "#### **Key Parameters Tuned**:  \n",
    "1. **Vectorizers (TF-IDF/BoW)**:  \n",
    "   - **`ngram_range`**: Considers unigrams `(1,1)` or unigrams + bigrams `(1,2)` to capture phrase-level patterns.  \n",
    "   - **`max_features`**: Limits the vocabulary size to 5,000 or 10,000 to balance model complexity and computational cost.  \n",
    "\n",
    "2. **Logistic Regression (LR)**:  \n",
    "   - **`C`**: Inverse regularization strength (`0.1`, `1`, `10`). Lower values increase regularization.  \n",
    "   - **`penalty`**: L1 (sparse solutions) or L2 (dense solutions) regularization.  \n",
    "\n",
    "3. **Support Vector Machine (SVM)**:  \n",
    "   - **`C`**: Regularization parameter to control margin width (`0.1`, `1`, `10`).  \n",
    "\n",
    "4. **Gradient Boosting (GB)**:  \n",
    "   - **`n_estimators`**: Number of trees (`100`, `200`). More trees reduce bias but risk overfitting.  \n",
    "   - **`learning_rate`**: Controls step size during boosting (`0.05`, `0.1`). Smaller values require more trees.  \n",
    "   - **`max_depth`**: Limits tree depth (`3`, `5`) to prevent overfitting.  \n",
    "   - **`min_samples_split`**: Minimum samples required to split a node (`2`, `5`).  \n",
    "   - **`max_features`**: Feature subsampling (`sqrt`, `log2`) to improve generalization.  \n",
    "\n",
    "5. **Multinomial Naive Bayes (MNB)**:  \n",
    "   - **`alpha`**: Laplace/Lidstone smoothing parameter (`0.1`, `1.0`, `10.0`). Controls prior probability adjustment.  \n",
    "   - **`fit_prior`**: Whether to learn class priors from data (`True`, `False`).  \n",
    "\n",
    "#### **Why These Parameters?**:  \n",
    "- **TF-IDF/BoW**: Balancing n-grams and vocabulary size ensures the model captures relevant patterns without overfitting.  \n",
    "- **Regularization (LR/SVM)**: Prevents overfitting on sparse text features.  \n",
    "- **Gradient Boosting**: Tree parameters control bias-variance tradeoff, while subsampling (`max_features`) reduces correlation between trees.  \n",
    "- **Naive Bayes**: Smoothing (`alpha`) handles zero-probability words, and `fit_prior` adjusts for class imbalance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'tfidf_lr': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__max_features': [5000, 10000],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'tfidf_svm': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__max_features': [5000, 10000],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'bow_lr': {\n",
    "        'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "        'bow__max_features': [5000, 10000],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'bow_svm': {\n",
    "        'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "        'bow__max_features': [5000, 10000],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'tfidf_gb': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__max_features': [5000, 10000],\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__learning_rate': [0.05, 0.1],\n",
    "        'clf__max_depth': [3, 5],\n",
    "        'clf__min_samples_split': [2, 5],\n",
    "        'clf__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'bow_gb': {\n",
    "        'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "        'bow__max_features': [5000, 10000],\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__learning_rate': [0.05, 0.1],\n",
    "        'clf__max_depth': [3, 5],\n",
    "        'clf__min_samples_split': [2, 5],\n",
    "        'clf__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'tfidf_mnb': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__max_features': [5000, 10000],\n",
    "        'clf__alpha': [0.1, 1.0, 10.0],\n",
    "        'clf__fit_prior': [True, False]\n",
    "    },\n",
    "    'bow_mnb': {\n",
    "        'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "        'bow__max_features': [5000, 10000],\n",
    "        'clf__alpha': [0.1, 1.0, 10.0],\n",
    "        'clf__fit_prior': [True, False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Cross-Validated Training & Hyperparameter Tuning**\n",
    "**Explanation**:  \n",
    "This step performs hyperparameter optimization using **GridSearchCV** with **StratifiedKFold** cross-validation to ensure class balance in imbalanced datasets.  \n",
    "\n",
    "#### **Key Components**:  \n",
    "1. **StratifiedKFold**:  \n",
    "   - **Why**: Maintains the original class distribution (disaster/non-disaster) in each fold to avoid biased evaluation.  \n",
    "   - **Parameters**: 5 splits, shuffled for randomness (`random_state=42`).  \n",
    "\n",
    "2. **GridSearchCV**:  \n",
    "   - **Purpose**: Exhaustively searches over specified hyperparameter combinations for each pipeline.  \n",
    "   - **Scoring**: Uses **F1-score** as the evaluation metric to balance precision and recall (critical for disaster classification where false negatives/positives are costly).  \n",
    "   - **Parallel Processing**: `n_jobs=-1` leverages all CPU cores for faster computation.  \n",
    "\n",
    "3. **Validation Process**:  \n",
    "   - After fitting each grid search, the **best estimator** (highest F1-score on validation folds) is selected.  \n",
    "   - The model is then evaluated on the **hold-out validation set** (`X_val_raw`, `y_val`) to compute the final validation score.  \n",
    "   - Results are stored in `best_models` for later comparison.  \n",
    "\n",
    "#### **Why This Approach Works**:  \n",
    "- **Hyperparameter Tuning**: Grid search ensures optimal parameter selection for each model-encoder combination.  \n",
    "- **Cross-Validation**: Reduces overfitting risk by averaging performance across folds.  \n",
    "- **F1-Score as Metric**: Prioritizes balanced performance in skewed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training TFIDF_LR ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Val F1: 0.7753\n",
      "Best Params: {'clf__C': 1, 'clf__penalty': 'l2', 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training TFIDF_SVM ===\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Val F1: 0.7783\n",
      "Best Params: {'clf__C': 0.1, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training BOW_LR ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Val F1: 0.7653\n",
      "Best Params: {'bow__max_features': 10000, 'bow__ngram_range': (1, 1), 'clf__C': 0.1, 'clf__penalty': 'l2'}\n",
      "\n",
      "=== Training BOW_SVM ===\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Val F1: 0.7627\n",
      "Best Params: {'bow__max_features': 10000, 'bow__ngram_range': (1, 1), 'clf__C': 0.1}\n",
      "\n",
      "=== Training TFIDF_GB ===\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Best Val F1: 0.7029\n",
      "Best Params: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 5, 'clf__n_estimators': 200, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training BOW_GB ===\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Best Val F1: 0.7083\n",
      "Best Params: {'bow__max_features': 5000, 'bow__ngram_range': (1, 2), 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 2, 'clf__n_estimators': 200}\n",
      "\n",
      "=== Training TFIDF_MNB ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Val F1: 0.7659\n",
      "Best Params: {'clf__alpha': 1.0, 'clf__fit_prior': False, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training BOW_MNB ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Val F1: 0.7753\n",
      "Best Params: {'bow__max_features': 5000, 'bow__ngram_range': (1, 1), 'clf__alpha': 1.0, 'clf__fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name in get_pipelines().keys():\n",
    "    print(f\"\\n=== Training {name.upper()} ===\")\n",
    "    pipeline = get_pipelines()[name]\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grids[name],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train_raw, y_train)\n",
    "\n",
    "    val_preds = grid_search.best_estimator_.predict(X_val_raw)\n",
    "    val_score = f1_score(y_val, val_preds)\n",
    "\n",
    "    best_models[name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'val_score': val_score,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "    print(f\"Best Val F1: {val_score:.4f}\")\n",
    "    print(f\"Best Params: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Final Model Selection & Kaggle Submission**\n",
    "**Explanation**:  \n",
    "- **Goal**: For each model type (e.g., TF-IDF SVM, BOW MNB, etc.), generate a Kaggle submission file using the best hyperparameter combination for that model type.  \n",
    "- **Key Steps**:  \n",
    "  1. **Model Selection**: For each model type, select the pipeline with the highest validation F1-score (from `best_models`).  \n",
    "  2. **Retraining**: Train the selected model on the **entire training dataset** (not just the training split) to utilize all available data.  \n",
    "  3. **Prediction**: Generate predictions for the test set using the cleaned text (`test_df['cleaned']`).  \n",
    "  4. **Submission File**: Save predictions in the required format (`id`, `target`) for Kaggle.  \n",
    "\n",
    "#### **Why This Approach Works**:  \n",
    "- **Per-Model Optimization**: Each model type is optimized independently, ensuring the best performance for each approach.  \n",
    "- **Full Dataset Training**: Improves model performance by leveraging all training samples.  \n",
    "- **Kaggle Submission Format**: Matches the required schema (`id`, `target`), ensuring compatibility with the competition's evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training and Submitting TFIDF_LR ===\n",
      "Submission file saved: tfidf_lr_submission.csv\n",
      "Best Validation F1: 0.7753\n",
      "Best Hyperparameters: {'clf__C': 1, 'clf__penalty': 'l2', 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training and Submitting TFIDF_SVM ===\n",
      "Submission file saved: tfidf_svm_submission.csv\n",
      "Best Validation F1: 0.7783\n",
      "Best Hyperparameters: {'clf__C': 0.1, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training and Submitting BOW_LR ===\n",
      "Submission file saved: bow_lr_submission.csv\n",
      "Best Validation F1: 0.7653\n",
      "Best Hyperparameters: {'bow__max_features': 10000, 'bow__ngram_range': (1, 1), 'clf__C': 0.1, 'clf__penalty': 'l2'}\n",
      "\n",
      "=== Training and Submitting BOW_SVM ===\n",
      "Submission file saved: bow_svm_submission.csv\n",
      "Best Validation F1: 0.7627\n",
      "Best Hyperparameters: {'bow__max_features': 10000, 'bow__ngram_range': (1, 1), 'clf__C': 0.1}\n",
      "\n",
      "=== Training and Submitting TFIDF_GB ===\n",
      "Submission file saved: tfidf_gb_submission.csv\n",
      "Best Validation F1: 0.7029\n",
      "Best Hyperparameters: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 5, 'clf__n_estimators': 200, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training and Submitting BOW_GB ===\n",
      "Submission file saved: bow_gb_submission.csv\n",
      "Best Validation F1: 0.7083\n",
      "Best Hyperparameters: {'bow__max_features': 5000, 'bow__ngram_range': (1, 2), 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 2, 'clf__n_estimators': 200}\n",
      "\n",
      "=== Training and Submitting TFIDF_MNB ===\n",
      "Submission file saved: tfidf_mnb_submission.csv\n",
      "Best Validation F1: 0.7659\n",
      "Best Hyperparameters: {'clf__alpha': 1.0, 'clf__fit_prior': False, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "=== Training and Submitting BOW_MNB ===\n",
      "Submission file saved: bow_mnb_submission.csv\n",
      "Best Validation F1: 0.7753\n",
      "Best Hyperparameters: {'bow__max_features': 5000, 'bow__ngram_range': (1, 1), 'clf__alpha': 1.0, 'clf__fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_info in best_models.items():\n",
    "    print(f\"\\n=== Training and Submitting {model_name.upper()} ===\")\n",
    "    \n",
    "    best_model = model_info['model']\n",
    "    best_model.fit(train_df['cleaned'], train_df['target'])\n",
    "    \n",
    "    test_preds = best_model.predict(test_df['cleaned'])\n",
    "    \n",
    "    submission_file = f'{model_name}_submission.csv'\n",
    "    pd.DataFrame({'id': test_df['id'], 'target': test_preds}).to_csv(submission_file, index=False)\n",
    "    print(f\"Submission file saved: {submission_file}\")\n",
    "    print(f\"Best Validation F1: {model_info['val_score']:.4f}\")\n",
    "    print(f\"Best Hyperparameters: {model_info['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Results Analysis & Conclusion**\n",
    "**Explanation**:  \n",
    "This section summarizes the performance of all models, highlights the best-performing pipeline for each model type, and provides actionable insights for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Performance Summary**:  \n",
    "| Pipeline          | Best Val F1-Score | Best Parameters                                                                 |  \n",
    "|-------------------|-------------------|---------------------------------------------------------------------------------|  \n",
    "| **TF-IDF SVM**    | **0.7783**        | `tfidf__max_features=10000`, `tfidf__ngram_range=(1,1)`, `clf__C=0.1`         |  \n",
    "| TF-IDF LR         | 0.7753           | `tfidf__max_features=10000`, `clf__C=1`, `clf__penalty='l2'`                   |  \n",
    "| BOW MNB           | 0.7753           | `bow__max_features=5000`, `clf__alpha=1.0`, `clf__fit_prior=True`              |  \n",
    "| BOW SVM           | 0.7627           | `bow__max_features=10000`, `clf__C=0.1`                                        |  \n",
    "| TF-IDF MNB        | 0.7659           | `tfidf__max_features=5000`, `clf__alpha=1.0`, `clf__fit_prior=False`           |  \n",
    "| BOW LR            | 0.7653           | `bow__max_features=10000`, `clf__C=0.1`, `clf__penalty='l2'`                   |  \n",
    "| TF-IDF GB         | 0.7152           | `clf__n_estimators=200`, `clf__learning_rate=0.1`, `tfidf__ngram_range=(1,2)` |  \n",
    "| BOW GB            | 0.7030           | `clf__n_estimators=200`, `clf__learning_rate=0.1`, `bow__ngram_range=(1,1)`   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Findings**:  \n",
    "1. **Best Model**:  \n",
    "   - **TF-IDF SVM** achieved the highest validation F1-score (**0.7783**).  \n",
    "   - Benefits from:  \n",
    "     - **TF-IDF Vectorization**: Captures term importance effectively.  \n",
    "     - **SVM with Low C**: Balances margin width to avoid overfitting.  \n",
    "\n",
    "2. **Gradient Boosting Limitations**:  \n",
    "   - Both TF-IDF and BOW variants underperformed (≤0.72 F1-score).  \n",
    "   - Likely due to:  \n",
    "     - High dimensionality of text data.  \n",
    "     - Tree-based models struggling with sparse features.  \n",
    "\n",
    "3. **Naive Bayes Competitiveness**:  \n",
    "   - BOW MNB scored **0.7753**, showing effectiveness for text tasks with probabilistic assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Recommendations for Improvement**:  \n",
    "- **Feature Engineering**:  \n",
    "  - Explore **n-gram combinations** (e.g., `(1,3)` for phrases) in TF-IDF.  \n",
    "  - Experiment with **custom tokenization** (e.g., handling emojis or hashtags explicitly).  \n",
    "- **Model Tweaks**:  \n",
    "  - For SVM: Try **non-linear kernels** (e.g., RBF) with dimensionality reduction (PCA/Truncated SVD).  \n",
    "  - For Gradient Boosting: Reduce depth (`max_depth=3`) and increase regularization.\n",
    "- **Ensemble Methods**:  \n",
    "  - Combine top models (e.g., TF-IDF SVM + BOW MNB) via stacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kaggle Submissions**:  \n",
    "- **TF-IDF SVM**: `tfidf_svm_submission.csv`  \n",
    "- **TF-IDF LR**: `tfidf_lr_submission.csv`  \n",
    "- **BOW MNB**: `bow_mnb_submission.csv`  \n",
    "- **BOW SVM**: `bow_svm_submission.csv`  \n",
    "- **TF-IDF MNB**: `tfidf_mnb_submission.csv`  \n",
    "- **BOW LR**: `bow_lr_submission.csv`  \n",
    "- **TF-IDF GB**: `tfidf_gb_submission.csv`  \n",
    "- **BOW GB**: `bow_gb_submission.csv`  \n",
    "\n",
    "Each submission file uses the best hyperparameter combination for its respective model type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
